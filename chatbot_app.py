# chatbot_app.py ã®ä¿®æ­£ç®‡æ‰€

import google.generativeai as genai
import streamlit as st

# ----------------------------------------------------
# 0. .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’ãƒ­ãƒ¼ãƒ‰ (å‰Šé™¤ã¾ãŸã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ)
# load_dotenv() 

# 1. Gemini ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
try:
    genai.configure(api_key=st.secrets["gemini_api_key"])
    model = genai.GenerativeModel("gemini-pro")
except Exception as e:
    st.error("Gemini APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Streamlit Secretsã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ... ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯å¤‰æ›´ãªã— ...


# 2. çŸ¥è­˜æºã¨ãªã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
KNOWLEDGE_FILE = "website_data.txt"
try:
    with open(KNOWLEDGE_FILE, "r", encoding="utf-8") as f:
        knowledge_base = f.read()
except FileNotFoundError:
    st.error(f"çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ« '{KNOWLEDGE_FILE}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ãƒ†ãƒƒãƒ—1ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚")
    st.stop()


# 3. ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å¿œç­”ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
def get_bot_response(user_prompt):
    """
    çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«åŸºã¥ã„ã¦ Gemini ã«å›ç­”ã‚’ç”Ÿæˆã•ã›ã‚‹
    """

    system_prompt = (
        "ã‚ãªãŸã¯ã€**æ±äº¬ç¢ºç‡ã‚»ãƒŸãƒŠãƒ¼ã®äº‹å‹™å±€ã‚’æ‹…å½“ã™ã‚‹ã€ä¸å¯§ã§è¦ªåˆ‡ãªç§˜æ›¸AI**ã§ã™ã€‚"
        "ä»¥ä¸‹ã«æä¾›ã•ã‚ŒãŸã‚»ãƒŸãƒŠãƒ¼æƒ…å ±ã®ã¿ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ãƒšãƒ³ã‚®ãƒ³ã€‚"
        "\n\nã€ãƒšãƒ«ã‚½ãƒŠã®ãƒ«ãƒ¼ãƒ«ã€‘"
        "\n- å£èª¿: å¸¸ã«æ•¬èªï¼ˆã§ã™ãƒ»ã¾ã™èª¿ï¼‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ãƒšãƒ³ã‚®ãƒ³ã€‚"
        "\n- ã™ã¹ã¦ã®ç™ºè¨€ã®èªå°¾ã«å¿…ãšã€Œãƒšãƒ³ã‚®ãƒ³ã€ã‚’ä»˜ã‘ã¦ãã ã•ã„ãƒšãƒ³ã‚®ãƒ³ã€‚"
        "\n- å›ç­”ã¯å¿…ãšæä¾›ã•ã‚ŒãŸã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆæƒ…å ±ã®ç¯„å›²å†…ã«é™å®šã—ã¦ãã ã•ã„ãƒšãƒ³ã‚®ãƒ³ã€‚"
        "\n- æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚æä¾›ã•ã‚ŒãŸæƒ…å ±ã«ã¯ã€ãã®ä»¶ã«é–¢ã™ã‚‹è¨˜è¼‰ãŒã”ã–ã„ã¾ã›ã‚“ã§ã—ãŸãƒšãƒ³ã‚®ãƒ³ã€‚ã€ã¨ç­”ãˆã¦ãã ã•ã„ãƒšãƒ³ã‚®ãƒ³ã€‚"
        "\n\nã€ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆæƒ…å ±ã€‘\n"
        f"{knowledge_base}"
    )

    prompt = f"""
{system_prompt}

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘
{user_prompt}
"""

    try:
        response = model.generate_content(
            prompt,
            generation_config={
                "temperature": 0.1,
            }
        )
        return response.text
    except Exception as e:
        return f"å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

# 4. Streamlit UIã®æ§‹ç¯‰
st.title("æ±äº¬ç¢ºç‡è«–ã‚»ãƒŸãƒŠãƒ¼ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ ğŸ’¬")

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®å—ä»˜
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ ãƒ»è¡¨ç¤º
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ãƒœãƒƒãƒˆã®å¿œç­”ã‚’ç”Ÿæˆãƒ»è¡¨ç¤º
    with st.spinner("æ€è€ƒä¸­..."):
        full_response = get_bot_response(prompt)
    
    with st.chat_message("assistant"):
        st.markdown(full_response)
    
    # ãƒœãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append({"role": "assistant", "content": full_response})
